[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is version 1 of the Responsible Research Metrics Self-Study module. The module was written by the Bibliometrics Team at the University of Southampton Library in 2024 and released in February 2025.\nThe module team is;\n\nKate Lapage\nClare Hemmings\nLorrayne Smith\n\nThe module’s foundations are taken from the university’s Responsible Research Policy. For help, feedback or suggestions please contact the team via eprints@soton.ac.uk.\nLinks provided to sites external to the University of Southampton were accurate at the time of release.\nThis module is shared under a CC BY license so that it may be adapted to suit other institutions’ policies. We ask that this page remains with further information by the adapting organisation added beneath. The module has been developed in R using Quarto and is available on GitHub for remixing https://github.com/UniSotonLibrary/responsible-metrics/\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "Section 3.html",
    "href": "Section 3.html",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nanyone who is interested in understanding author metrics;\nanyone who is applying for job roles, promotion cycles or applying for funding/grant applications where research may be used as an assessment tool.\n\nIn this section you will explore: \n\nmetrics in the context of individual research;\nsources of metrics;\nlimitation of metrics;\nguidance on how to use metrics.\n\n\n\nMetrics are a quantitative snapshot (i.e. indicators) of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics, and if you can evidence the impact or your contribution without using metrics, you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for your article to be indexed (sometimes even longer!).\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.\n\nCitations take a long time to accumulate.\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\n\nYou could use metrics to evidence statements you make about your research.\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you use metrics you need to use them responsibly, for example, using the correct type of metric for your purpose. \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo, how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers.\nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content.\nAltmetrics – metrics derived from alternative types of research output, such as citations or acknowledgements on a public-facing website, social media engagement, or attention from the news media are particularly useful when assessing impact.\nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your own research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is rigorous and reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 3.html#section-3---using-metrics-in-personal-applications-and-evaluations",
    "href": "Section 3.html#section-3---using-metrics-in-personal-applications-and-evaluations",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nanyone who is interested in understanding author metrics;\nanyone who is applying for job roles, promotion cycles or applying for funding/grant applications where research may be used as an assessment tool.\n\nIn this section you will explore: \n\nmetrics in the context of individual research;\nsources of metrics;\nlimitation of metrics;\nguidance on how to use metrics.\n\n\n\nMetrics are a quantitative snapshot (i.e. indicators) of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics, and if you can evidence the impact or your contribution without using metrics, you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for your article to be indexed (sometimes even longer!).\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.\n\nCitations take a long time to accumulate.\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\n\nYou could use metrics to evidence statements you make about your research.\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you use metrics you need to use them responsibly, for example, using the correct type of metric for your purpose. \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo, how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers.\nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content.\nAltmetrics – metrics derived from alternative types of research output, such as citations or acknowledgements on a public-facing website, social media engagement, or attention from the news media are particularly useful when assessing impact.\nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your own research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is rigorous and reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 1.html",
    "href": "Section 1.html",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nwhat metrics are in the context of research;\nthe principles of responsible metrics;\nguidelines around the responsible use of metrics.\n\n\n\nMetrics are a quantitative snapshot of how research outputs perform. A research output is anything which disseminates research; for instance a journal article, a book, an exhibition or software. Metrics can look at one research output, at the outputs from a single researcher, or at the entire output of a department or University. Other metrics indicate the relevance and quality of a venue, such as a journal, but these are not appropriate indicators to use as a proxy to assess an individual or their output. The majority of metrics primarily focus on journal articles but metrics may be available for other research output types.\nThere are many different types of metrics and they each have a prescribed purpose.\nWhen using metrics it is important to have a question you are trying to answer so you can decide which metrics to use.\nUsing the wrong research metric can have real world implications, from making false claims to causing reputational harm, which could impact future career ambitions or cause institutional damage.\n\n\n\nAs research disciplines and outputs are so varied there is no set step-by-step guide to carrying out an analysis. Instead, there are a set of principles that should be followed: Transparency, Appropriateness, Equality, Reproducible, and Continual Reassessment.\n\n\nTransparency in metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nEssentially what, where and when: \n\nWhat metrics were used? \nWhere did the data come from (sources)? \nWhen was the assessment made? – The assessment is only a snapshot in time; data sources are updated as indicators mature. \n\n\n\n\nThe use of metrics should be fair.  You should only compare like-for-like i.e. make comparisons within a single discipline, a limited time period, or using a normalised metric.\nConsistency is important. If you can’t apply the same method across your analysis you should not use that method. \nBe aware that individuals may not be directly comparable due to length of service or time out of work, such as for maternity leave.\n\nE.g. you should not compare an early career researcher with a researcher who has 20 years of experience unless you have restricted the comparison to a suitable time frame. \n\n\n\n\nAll metrics should be tailored to the focus of the analysis. Use metrics for their intended purpose only.\n\nFor example, you must not use a journal metric to infer the quality of an individual output or a person’s contribution to research.\n\nMetrics should only be used when necessary and should be used in conjunction with expert testimony rather than in isolation. \nWhen assessing a person, metrics must not be used as the sole source of information. This is especially true for employment status, but also for personal reputation in a formal or informal context. \n\nFor example, a highly cited paper might be highly cited because everyone disagrees with it\n\nIt is recommended that you use more than one metric to verify results. \n\n\n\nAnyone should be able to reproduce your results by using the explanation you have provided.\n\n\n\nContinually assess commonly used metrics, especially concerning appropriateness and equality. If a metric is no longer fit for purpose, it should not be used. \n\n\n\n\nA number of guidelines have been published which look to shape our approach to responsible metrics.  The University has its own responsible metrics policy which is based on the principles of DORA and the Leiden Manifesto (https://doi.org/10.1038/520429a). Below are summaries of three key international initiatives, each of which advise on a different aspect: \nDORA (the Declaration of Research Assessment) \nDORA is a set of principles that were published in 2012. They are designed to ensure that the quality and impact of scientific outputs is “measured accurately and evaluated wisely”.\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nThe University of Southampton is a signatory of DORA.\nCoARA (Coalition of Advancing Research Assessment) \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives. Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research. This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators” (metrics). \nBarcelona Declaration on Open Research Information \nLaunched in April 2024, the Barcelona Declaration has the backing of research and funding organisations who have endorsed its commitment to “make openness the default for the research information we use and produce”. \nOne of its central recommendations is that we should move away from the use of closed and commercial data sources and work towards services and systems that support and enable research information which is open. These open tools should then give access to metrics that have greater transparency and reproducibility. \n\n\n\nMetrics are used in a wide range of activities such as:\n\nresearch assessment;\ngrant applications;\nrecruitment;\npromotion;\nleague tables, such as QS.\n\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 2 - Using metrics in research assessment.\nSection 3 - Using metrics in personal applications and evaluations.\nSection 4 - Assessing people using metrics.\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about.\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4.",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 1.html#section-1---responsible-research-metrics-overview",
    "href": "Section 1.html#section-1---responsible-research-metrics-overview",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nwhat metrics are in the context of research;\nthe principles of responsible metrics;\nguidelines around the responsible use of metrics.\n\n\n\nMetrics are a quantitative snapshot of how research outputs perform. A research output is anything which disseminates research; for instance a journal article, a book, an exhibition or software. Metrics can look at one research output, at the outputs from a single researcher, or at the entire output of a department or University. Other metrics indicate the relevance and quality of a venue, such as a journal, but these are not appropriate indicators to use as a proxy to assess an individual or their output. The majority of metrics primarily focus on journal articles but metrics may be available for other research output types.\nThere are many different types of metrics and they each have a prescribed purpose.\nWhen using metrics it is important to have a question you are trying to answer so you can decide which metrics to use.\nUsing the wrong research metric can have real world implications, from making false claims to causing reputational harm, which could impact future career ambitions or cause institutional damage.\n\n\n\nAs research disciplines and outputs are so varied there is no set step-by-step guide to carrying out an analysis. Instead, there are a set of principles that should be followed: Transparency, Appropriateness, Equality, Reproducible, and Continual Reassessment.\n\n\nTransparency in metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nEssentially what, where and when: \n\nWhat metrics were used? \nWhere did the data come from (sources)? \nWhen was the assessment made? – The assessment is only a snapshot in time; data sources are updated as indicators mature. \n\n\n\n\nThe use of metrics should be fair.  You should only compare like-for-like i.e. make comparisons within a single discipline, a limited time period, or using a normalised metric.\nConsistency is important. If you can’t apply the same method across your analysis you should not use that method. \nBe aware that individuals may not be directly comparable due to length of service or time out of work, such as for maternity leave.\n\nE.g. you should not compare an early career researcher with a researcher who has 20 years of experience unless you have restricted the comparison to a suitable time frame. \n\n\n\n\nAll metrics should be tailored to the focus of the analysis. Use metrics for their intended purpose only.\n\nFor example, you must not use a journal metric to infer the quality of an individual output or a person’s contribution to research.\n\nMetrics should only be used when necessary and should be used in conjunction with expert testimony rather than in isolation. \nWhen assessing a person, metrics must not be used as the sole source of information. This is especially true for employment status, but also for personal reputation in a formal or informal context. \n\nFor example, a highly cited paper might be highly cited because everyone disagrees with it\n\nIt is recommended that you use more than one metric to verify results. \n\n\n\nAnyone should be able to reproduce your results by using the explanation you have provided.\n\n\n\nContinually assess commonly used metrics, especially concerning appropriateness and equality. If a metric is no longer fit for purpose, it should not be used. \n\n\n\n\nA number of guidelines have been published which look to shape our approach to responsible metrics.  The University has its own responsible metrics policy which is based on the principles of DORA and the Leiden Manifesto (https://doi.org/10.1038/520429a). Below are summaries of three key international initiatives, each of which advise on a different aspect: \nDORA (the Declaration of Research Assessment) \nDORA is a set of principles that were published in 2012. They are designed to ensure that the quality and impact of scientific outputs is “measured accurately and evaluated wisely”.\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nThe University of Southampton is a signatory of DORA.\nCoARA (Coalition of Advancing Research Assessment) \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives. Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research. This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators” (metrics). \nBarcelona Declaration on Open Research Information \nLaunched in April 2024, the Barcelona Declaration has the backing of research and funding organisations who have endorsed its commitment to “make openness the default for the research information we use and produce”. \nOne of its central recommendations is that we should move away from the use of closed and commercial data sources and work towards services and systems that support and enable research information which is open. These open tools should then give access to metrics that have greater transparency and reproducibility. \n\n\n\nMetrics are used in a wide range of activities such as:\n\nresearch assessment;\ngrant applications;\nrecruitment;\npromotion;\nleague tables, such as QS.\n\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 2 - Using metrics in research assessment.\nSection 3 - Using metrics in personal applications and evaluations.\nSection 4 - Assessing people using metrics.\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about.\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4.",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Responsible Research Metrics Self-Study Module",
    "section": "",
    "text": "Welcome to this self-study module\nThis self-study module covers the core principles of responsible research metrics and can be completed in your own time.\nThis module is designed for:\n\nAnyone who is interested in how research is measured and assessed;\nAnyone involved in recruitment, reward and recognition processes or grant applications where research may be used as an assessment tool.\n\nEveryone should complete Section 1, 2, and 3. Section 4 is for those who are involved in recruitment, reward and recognition processes, or grant applications as an assessor of people.\nFor help please contact the Library metrics team via eprints@soton.ac.uk\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Section 2.html",
    "href": "Section 2.html",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nanyone who is interested in how metrics contribute to research assessment;\nanyone who is involved in assessing research where metrics may be used as an assessment tool.\n\nIn this this section you will explore:\n\nguidance on how to use metrics in research assessment;\nan example question.\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions:\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you are trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create data visualisations for you, such as VOSviewer.\nWhatever format you decide on, it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued.\nIt’s important to be clear and transparent about the sources you use, so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you made about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you have used it.\n\n\n\nSo, how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with, we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information to do a like-for-like comparison.\nYou will need to be conscious of:\n\nDate i.e. is one paper newer?\nOpen access status – can one paper be accessed by a wider potential audience?\nJournal – if you unable to explain a difference in metrics between two outputs using the information you have, you may want to look at the publication. For example, it may be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y, so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we cannot tell the impact of a paper from citation count alone, so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach too. The question you ask will determine the answer you get. So, take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 2.html#section-2---using-metrics-in-research-assessment",
    "href": "Section 2.html#section-2---using-metrics-in-research-assessment",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nanyone who is interested in how metrics contribute to research assessment;\nanyone who is involved in assessing research where metrics may be used as an assessment tool.\n\nIn this this section you will explore:\n\nguidance on how to use metrics in research assessment;\nan example question.\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions:\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you are trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create data visualisations for you, such as VOSviewer.\nWhatever format you decide on, it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued.\nIt’s important to be clear and transparent about the sources you use, so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you made about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you have used it.\n\n\n\nSo, how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with, we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information to do a like-for-like comparison.\nYou will need to be conscious of:\n\nDate i.e. is one paper newer?\nOpen access status – can one paper be accessed by a wider potential audience?\nJournal – if you unable to explain a difference in metrics between two outputs using the information you have, you may want to look at the publication. For example, it may be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y, so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we cannot tell the impact of a paper from citation count alone, so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach too. The question you ask will determine the answer you get. So, take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 4.html",
    "href": "Section 4.html",
    "title": "Section 4",
    "section": "",
    "text": "This section is designed for:\n\nanyone interested in how author metrics contribute to research assessment;\nanyone who undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool.\n\nIn this section you will explore:\n\nhow to use metrics in your assessment;\nlimitations of metrics.\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing.\nSpecify a particular source to use or a specific metrics to use.\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates.\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss.\n\n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for articles to be indexed.\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accrue, favouring older papers over new.\nLack of context:\n\nthere is no way of knowing if citations are positive or negative;\nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not.\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage.\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution.\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal metric to infer the quality onto their article.\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is more typical.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section. This is the last part of this self-study module.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 4.html#section-4---assessing-people-using-metrics",
    "href": "Section 4.html#section-4---assessing-people-using-metrics",
    "title": "Section 4",
    "section": "",
    "text": "This section is designed for:\n\nanyone interested in how author metrics contribute to research assessment;\nanyone who undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool.\n\nIn this section you will explore:\n\nhow to use metrics in your assessment;\nlimitations of metrics.\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing.\nSpecify a particular source to use or a specific metrics to use.\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates.\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss.\n\n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for articles to be indexed.\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accrue, favouring older papers over new.\nLack of context:\n\nthere is no way of knowing if citations are positive or negative;\nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not.\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage.\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution.\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal metric to infer the quality onto their article.\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is more typical.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section. This is the last part of this self-study module.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  }
]